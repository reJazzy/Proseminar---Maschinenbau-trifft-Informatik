
@misc{robotics_international_nodate,
	title = {International {Federation} of {Robotics}},
	url = {https://ifr.org},
	abstract = {The International Federation of Robotics is a professional non-profit organization to promote, strengthen and protect the robotics industry worldwide.},
	language = {en},
	urldate = {2026-01-07},
	journal = {IFR International Federation of Robotics},
	author = {Robotics, IFR International Federation of},
	file = {Snapshot:C\:\\Users\\Jesse\\Zotero\\storage\\TI4FI6QK\\ifr.org.html:text/html},
}

@article{liu_vision_2025,
	title = {Vision intelligence-conditioned reinforcement learning for precision assembly},
	volume = {74},
	issn = {0007-8506},
	url = {https://www.sciencedirect.com/science/article/pii/S0007850625000642},
	doi = {10.1016/j.cirp.2025.04.016},
	abstract = {Robots that embrace human-level performance on precise, dexterous and dynamic assembly tasks can significantly enhance the efficiency in precision assembly but remain big challenges. This paper introduces a vision intelligence-conditioned method for precision assembly, enabled by human-in-the-loop reinforcement learning. Upon visual demonstrations collected and trained by a reward classifier, a data-efficient reinforcement learning algorithm trains and learns vision-based robotic manipulation policies under human-in-the-loop corrections. An impedance-based control strategy derived from policies and visual guidance achieves high-precision contact-rich assembly manipulations with near-perfect success rates (above 98\%) and compliance behaviours. The effectiveness of the presented method is experimentally demonstrated with semiconductor assembly.},
	number = {1},
	urldate = {2026-01-07},
	journal = {CIRP Annals},
	author = {Liu, Sichao and Wang, Lihui},
	month = jan,
	year = {2025},
	keywords = {Assembly, Reinforcement learning, Robot},
	pages = {13--17},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Jesse\\Zotero\\storage\\NRR4DALI\\Liu und Wang - 2025 - Vision intelligence-conditioned reinforcement learning for precision assembly.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Jesse\\Zotero\\storage\\MYPW6GCA\\S0007850625000642.html:text/html},
}

@misc{google_deepmind_rl_2015,
	title = {{RL} {Course} by {David} {Silver} - {Lecture} 2: {Markov} {Decision} {Process}},
	shorttitle = {{RL} {Course} by {David} {Silver} - {Lecture} 2},
	url = {https://www.youtube.com/watch?v=lfHX2hHRMVQ},
	abstract = {\#Reinforcement Learning Course by David Silver\# Lecture 2:  Markov Decision Process

\#Slides and more info about the course: http://goo.gl/vUiyjq},
	urldate = {2026-01-07},
	author = {{Google DeepMind}},
	month = may,
	year = {2015},
}

@misc{noauthor_kr_nodate,
	title = {{KR} {QUANTEC}},
	url = {https://www.kuka.com/de-de/produkte-leistungen/robotersysteme/industrieroboter/kr-quantec},
	abstract = {Allround-Roboterserie mit größtem Traglast- und Reichweiten-Portfolio in hoher Traglastklasse: für Einsätze z. B. in Autoindustrie, Gießerei, Medizin},
	language = {de-DE},
	urldate = {2026-01-07},
	journal = {KUKA},
}

@misc{noauthor_was_2021,
	title = {Was ist ein neuronales {Netz}? {\textbar} {IBM}},
	shorttitle = {Was ist ein neuronales {Netz}?},
	url = {https://www.ibm.com/de-de/think/topics/neural-networks},
	abstract = {Neuronale Netzwerke ermöglichen es Programmen, Muster zu erkennen und allgemeine Probleme in den Bereichen künstliche Intelligenz, maschinelles Lernen und Deep Learning zu lösen.},
	language = {de},
	urldate = {2026-01-08},
	month = oct,
	year = {2021},
	file = {Snapshot:C\:\\Users\\Jesse\\Zotero\\storage\\AUIXATV6\\neural-networks.html:text/html},
}

@misc{haarnoja_soft_2018,
	title = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
	shorttitle = {Soft {Actor}-{Critic}},
	url = {http://arxiv.org/abs/1801.01290},
	doi = {10.48550/arXiv.1801.01290},
	abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	language = {en},
	urldate = {2026-01-08},
	publisher = {arXiv},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	month = aug,
	year = {2018},
	note = {arXiv:1801.01290 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICML 2018 Videos: sites.google.com/view/soft-actor-critic Code: github.com/haarnoja/sac},
	file = {PDF:C\:\\Users\\Jesse\\Zotero\\storage\\P3AYDPV7\\Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:application/pdf},
}

@article{ball_efficient_nodate,
	title = {Efficient {Online} {Reinforcement} {Learning} with {Offline} {Data}},
	abstract = {Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see that correct application of these simple recommendations can provide a 2.5× improvement over existing approaches across a diverse set of competitive benchmarks, with no additional computational overhead. We have released our code here: github.com/ikostrikov/rlpd.},
	language = {en},
	author = {Ball, Philip J and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
	file = {PDF:C\:\\Users\\Jesse\\Zotero\\storage\\REBNHVIG\\Ball et al. - Efficient Online Reinforcement Learning with Offline Data.pdf:application/pdf},
}

@article{tsividis_human_nodate,
	title = {Human {Learning} in {Atari}},
	abstract = {Atari games are an excellent testbed for studying intelligent behavior, as they offer a range of tasks that differ widely in their visual representation, game dynamics, and goals presented to an agent. The last two years have seen a spate of research into artiﬁcial agents that use a single algorithm to learn to play these games. The best of these artiﬁcial agents perform at better-than-human levels on most games, but require hundreds of hours of game-play experience to produce such behavior. Humans, on the other hand, can learn to perform well on these tasks in a matter of minutes. In this paper we present data on human learning trajectories for several Atari games, and test several hypotheses about the mechanisms that lead to such rapid learning.},
	language = {en},
	author = {Tsividis, Padro A and Pouncy, Thomas and Xu, Jacqueline L and Tenenbaum, Joshua B and Gershman, Samuel J},
	file = {PDF:C\:\\Users\\Jesse\\Zotero\\storage\\24W8H6LA\\Tsividis et al. - Human Learning in Atari.pdf:application/pdf},
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	copyright = {2016 Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	language = {en},
	number = {7587},
	urldate = {2026-01-09},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Computer science, Reward},
	pages = {484--489},
	file = {Full Text PDF:C\:\\Users\\Jesse\\Zotero\\storage\\J59K23JE\\Silver et al. - 2016 - Mastering the game of Go with deep neural networks and tree search.pdf:application/pdf},
}

@misc{noauthor_binary_nodate,
	title = {Binary {Classifier} - an overview {\textbar} {ScienceDirect} {Topics}},
	url = {https://www.sciencedirect.com/topics/computer-science/binary-classifier},
	urldate = {2026-01-09},
	file = {Binary Classifier - an overview | ScienceDirect Topics:C\:\\Users\\Jesse\\Zotero\\storage\\CRJPT5G3\\binary-classifier.html:text/html},
}

@incollection{thrun_issues_1994,
	title = {Issues in {Using} {Function} {Approximation} for {Reinforcement} {Learning}},
	abstract = {Reinforcement learning techniques address the problem of learning to select actions in unknown, dynamic environments. It is widely acknowledged that to be of use in complex domains, reinforcement learning techniques must be combined with generalizing function approximation methods such as artificial neural networks. Little, however, is understood about the theoretical properties of such combinations, and many researchers have encountered failures in practice. In this paper we identify a prime source of such failures—namely, a systematic overestimation of utility values. Using Walkins’ Q-Learning [18] as an example, we give a theoretical account of the phenomenon, deriving conditions under which one may expected it to cause learning to fail. Employing some of the most popular function approximators, we present experimental results which support the theoretical findings.},
	booktitle = {Proceedings of the 1993 {Connectionist} {Models} {Summer} {School}},
	publisher = {Psychology Press},
	author = {Thrun, Sebastian and Schwartz, Anton},
	year = {1994},
	note = {Num Pages: 9},
}

@article{noauthor_using_nodate,
	title = {Using {Fast} {Weights} to {Attend} to the {Recent} {Past}},
	abstract = {Until recently, research on artiﬁcial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artiﬁcial neural networks might beneﬁt from variables that change slower than activities but much faster than the standard weights. These “fast weights” can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.},
	language = {en},
	file = {PDF:C\:\\Users\\Jesse\\Zotero\\storage\\QVC8A79I\\Using Fast Weights to Attend to the Recent Past.pdf:application/pdf},
}

@book{berthold_intelligent_2007,
	title = {Intelligent {Data} {Analysis}: {An} {Introduction}},
	isbn = {978-3-540-48625-1},
	shorttitle = {Intelligent {Data} {Analysis}},
	abstract = {This monograph is a detailed introductory presentation of the key classes of intelligent data analysis methods. The twelve coherently written chapters by leading experts provide complete coverage of the core issues. The first half of the book is devoted to the discussion of classical statistical issues, ranging from the basic concepts of probability, through general notions of inference, to advanced multivariate and time series methods, as well as a detailed discussion of the increasingly important Bayesian approaches and Support Vector Machines. The following chapters then concentrate on the area of machine learning and artificial intelligence and provide introductions into the topics of rule induction methods, neural networks, fuzzy logic, and stochastic search methods. The book concludes with a chapter on Visualization and a higher-level overview of the IDA processes, which illustrates the breadth of application of the presented ideas.},
	language = {en},
	publisher = {Springer},
	author = {Berthold, Michael R. and Hand, David J.},
	month = jun,
	year = {2007},
	note = {Google-Books-ID: den1CAAAQBAJ},
	keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition, Computers / Artificial Intelligence / General, Computers / Computer Science, Computers / Data Science / Data Analytics, Computers / Information Technology, Computers / Software Development \& Engineering / General, Computers / System Administration / Storage \& Retrieval, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes},
}

@inproceedings{gong_resnet10_2022,
	title = {{ResNet10}: {A} lightweight residual network for remote sensing image classification},
	shorttitle = {{ResNet10}},
	url = {https://ieeexplore.ieee.org/document/9724001/},
	doi = {10.1109/ICMTMA54903.2022.00197},
	abstract = {Remote sensing image classification is challenging due to its complex background and various types. Currently, the commonly used lightweight neural network still has the problem of excessive calculation and parameter amount, which makes cheap mobile devices with low computing power unable to quickly complete remote sensing image classification tasks. In response to this problem, we propose a lightweight residual neural network-ResNet10 that is more suitable for use on cheap mobile devices with low computing power. Experiments based on the UCM Land-Use public data set show that the classification accuracy of ResNet10 can reach 96.2\%, the size of the model is one-tenth that of ResNetl8, and it can classify 105 remote sensing images per second. ResNet10 makes it possible for remote sensing image classification to be used on mobile devices.},
	urldate = {2026-01-10},
	booktitle = {2022 14th {International} {Conference} on {Measuring} {Technology} and {Mechatronics} {Automation} ({ICMTMA})},
	author = {Gong, Jiaming and Liu, Wei and Pei, Mengjie and Wu, Chengchao and Guo, Liufei},
	month = jan,
	year = {2022},
	note = {ISSN: 2157-1481},
	keywords = {Adaptation models, Computational modeling, Data models, Lightweight Neural Network, Mobile handsets, Neural networks, Performance evaluation, Remote Sensing Image, ResNet10, Task analysis},
	pages = {975--978},
	file = {Full Text PDF:C\:\\Users\\Jesse\\Zotero\\storage\\A7J968MF\\Gong et al. - 2022 - ResNet10 A lightweight residual network for remote sensing image classification.pdf:application/pdf},
}

@incollection{yau_estimation_2024,
	address = {Cham},
	title = {Estimation {Algorithms} {Based} on {Deep} {Learning}},
	isbn = {978-3-031-77684-7},
	url = {https://doi.org/10.1007/978-3-031-77684-7_10},
	abstract = {In this chapter, we shall first review the estimation problem from the perspective of the machine learning. Then we shall revisit the classical neural networks models including feedforward neural networks (FNNs) and recurrent neural networks (RNNs) in details. Specifically, we’re going to introduce the specific mathematical form of their network architecture. Then we shall discuss their approximation ability, the universal approximation theorem, and give an elegant proof. And we also introduce the mathematics on how to train the neural networks, i.e., the backpropagation algorithm and currently popular optimization algorithms for deep learning optimization problems. Finally, we shall introduce how to use deep learning method to solve state estimation problems, i.e., filtering problems.},
	language = {en},
	urldate = {2026-01-11},
	booktitle = {Principles of {Nonlinear} {Filtering} {Theory}},
	publisher = {Springer Nature Switzerland},
	author = {Yau, Stephen S. -T. and Chen, Xiuqiong and Jiao, Xiaopei and Kang, Jiayi and Sun, Zeju and Tao, Yangtianze},
	editor = {Yau, Stephen S.-T. and Chen, Xiuqiong and Jiao, Xiaopei and Kang, Jiayi and Sun, Zeju and Tao, Yangtianze},
	year = {2024},
	doi = {10.1007/978-3-031-77684-7_10},
	pages = {385--427},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2026-01-11},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Fixes a typo in the abstract, no other changes},
	file = {Preprint PDF:C\:\\Users\\Jesse\\Zotero\\storage\\NN2NWHEM\\Kingma und Welling - 2022 - Auto-Encoding Variational Bayes.pdf:application/pdf;Snapshot:C\:\\Users\\Jesse\\Zotero\\storage\\PA2KS9YV\\1312.html:text/html},
}

@misc{ross_agnostic_2012,
	title = {Agnostic {System} {Identification} for {Model}-{Based} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1203.1007},
	doi = {10.48550/arXiv.1203.1007},
	abstract = {A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis. To provide good performance guarantees, existing methods must assume that the real system is in the class of models considered during learning. We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class. In particular, we show that any no-regret online learning algorithm can be used to obtain a near-optimal policy, provided some model achieves low training error and access to a good exploration distribution. Our approach applies to both discrete and continuous domains. We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature.},
	urldate = {2026-01-11},
	publisher = {arXiv},
	author = {Ross, Stephane and Bagnell, J. Andrew},
	month = jul,
	year = {2012},
	note = {arXiv:1203.1007 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Statistics - Machine Learning},
	annote = {Comment: 8 pages, published in ICML 2012},
	file = {Preprint PDF:C\:\\Users\\Jesse\\Zotero\\storage\\XY8WTRR8\\Ross und Bagnell - 2012 - Agnostic System Identification for Model-Based Reinforcement Learning.pdf:application/pdf;Snapshot:C\:\\Users\\Jesse\\Zotero\\storage\\7QNH7YVZ\\1203.html:text/html},
}

@misc{ba_layer_2016,
	title = {Layer {Normalization}},
	url = {http://arxiv.org/abs/1607.06450},
	doi = {10.48550/arXiv.1607.06450},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	urldate = {2026-01-11},
	publisher = {arXiv},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
	month = jul,
	year = {2016},
	note = {arXiv:1607.06450 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Jesse\\Zotero\\storage\\GRYVTZVL\\Ba et al. - 2016 - Layer Normalization.pdf:application/pdf;Snapshot:C\:\\Users\\Jesse\\Zotero\\storage\\UQRCK4T2\\1607.html:text/html},
}

@misc{noauthor_what_2021,
	title = {What is {Overfitting}? {\textbar} {IBM}},
	shorttitle = {What is {Overfitting}?},
	url = {https://www.ibm.com/think/topics/overfitting},
	abstract = {Overfitting occurs when an algorithm fits too closely to its training data, resulting in a model that can’t make accurate predictions or conclusions.},
	language = {en},
	urldate = {2026-01-11},
	month = oct,
	year = {2021},
	file = {Snapshot:C\:\\Users\\Jesse\\Zotero\\storage\\UEEM3MHV\\overfitting.html:text/html},
}
